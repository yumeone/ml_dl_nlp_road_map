
## Transfomers

###  ```Resources :```

<a href = 'http://jalammar.github.io/illustrated-transformer/'>The Illustrated Transformer by Jay</a>

<a href = 'https://arxiv.org/pdf/1706.03762.pdf'> Attention all you need </a>


### Basics :

#### * Sequence transduction.

A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an imagesâ€¦etc) and outputs another sequence of items. A trained model would work like this,
* The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item


![img.png](https://github.com/Uttam580/ml_dl_nlp_road_map/blob/master/deep_learning2/basic_transfomers/gif/seq2seq.gif)
